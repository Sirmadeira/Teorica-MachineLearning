Nessa aula iremos visualizar como arvores de Xgboost de classificacao s√£o criadas.

O dataset utilizado e um de eficacia de uma droga. Ver imagen:(Dataset xgboost para classificacao)


O primeiro passo para encaixar xgboost a parte de treinamento desse dataset e, fazer uma predicao inicial.

A predicao inical, e um numero random geralmente 0.5. Nao importa se e classificacao ou regressao.

Esse numero sera o ponto de partida de tudo, incluido o calculo dos residuos dos dataset observados. No exemplo, as dosagens eficazes tem o valor 1, as ineficazes 0. 
Lembrete: Residuo=Dado observado-Dado predito

Agora como a gente fez na regressao, a gente treina uma arvore de xgboost para com os residuos.

No entanto dessa vez temos uma nova formulinha para calcular o Similiraty Score.

Similiraty Score= (Soma dos residuos) ao quadrado//
Soma[Probabilidade anterior*(1-Probabilidade anterior)]+lambda

A gente ja viu isso de probabilidade anterior em gradient boost normal. Mas so por recapitulacao seria a probabilidade predita anterior para o valor do datapoint analisado, lembre-se probablidade predita considera a passagem  pelas arvores passadas*learning rate + predicao inicial.
Lembrete: A soma acontece antes de pormos eles ao quadrado.

Exatamente como na regressao a gente comeca contruindo nossa arvore por somente um leaf node, com todos os residuos da predicao inicial dentro dele. No exemplo, o numerador e zerado. Logo a similaridade e 0 para o galho inicial. Ver imagen:(Calculo do xemplo dosagem classificacao inicial)

Agora temos que decidir, se podemos melhorar a nosso metodologia de clustering de residuos, se dividirmos em grupos.

Vamos comecar com aquela parte de selecao de limiares. Comecamos com o threshhold dos dois ultimos pontos no exemplo, esse vai originar a divisao entre os residuos. E calculamos o similarity score.
O lambda no exemplo e 0. 
Ver imagen:(Similarity score primeiro limiar arvore de classificacao)

E logo em seguida calculamos o gain. A formula e a mesma da de regressao.

Ganho(Gain)=Similarity do leaf node da esquerda+ Similidarity do leaf node da direita- Similaridade do root node.

No exemplo, nenhum dos outros limiares no root nade tem um gain maior.

Depois a gente faz o mesmo processo, nos residuos do leaf node esquerdo. 
Ver imagem:(Leaf node esquerdo arvore clasfficacao)

A gente para ae porque limitamos, o numero de niveis de leaf nodes em 2.

No entanto, XGboost tambem tem um numero minimo residuos que pode ter dentro de cada leaf node.
Esse numero e definido por algo chamado Cover.

