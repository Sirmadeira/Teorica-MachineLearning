Nas ultimas duas aulas, demonstrei como fazer xgboost para classificacao e regressao.

Nessa aula, eu vou demonstrar como a unica diferenca entre esse dois tipos de arvore ,matematicamente falando, e a funcao de perda(loss function).

Algo que vou demonstrar, comparando a derivada da funcao Similarity Score e a de Valor de output, das arvores de regressao com as das arvores de classificacao

O dataset utilizado para explicacao inicial sera esse. Ver imagen:(Dataset de explicacao xgboost passo a passo)

Como no gradient boost nao extremo, a loss function e utilizada para descobrir o quao bom nossa predicao e. 
Na regressao, essa funcao seria:

Ver imagen:(Funcao de loss regressao)

Lembrete: yi= Seria o valor y de um ponto observado, selecionado. No dataset, da imagen:(Dataset de explicacao xgboost passo a passo), o ponto y1 teria o valor de -10. pi seria o valor da predicao final para um ponto especifico.p1, seria a predicao final para o ponto do datapoint 1.

Para melhor entendimento veja a resulucao da loss function, quando a nossa predicao final = a predicao inicial.Ver imagen:(Formula com pi = a predicao inicial).

A gente, no futuro pode se utilizar dessa loss function, para verificar se as predicoes sucedentes dado o avanco da construcao das arvores xgboost, estao melhorando.
 
A loss function para classificacao e:

Ver imagen:(Funcao de loss classificacao)

Lembrete: Na classificao y tem valores entre 1 e 0 porque ela e classificacao, no nossa dataset 1 e eficaz 0 e nao eficaz. Logo nao estranhe se o valor de, yi(tanto faz o datapoint) tiver esses valores.


Xgboost se utiliza dessas loss functions demonstradas, para construir arvores, que tem como objetivo, minimizar a seguinte equacao. Atraves dos valores de output.

Ver imagen:(Equacao de construcao de arvores)

Nota: Esse y estranho com o T no meio da somacao, vai ser desconsiderado.
Porque ele representa, o estimulo do processo de pruning. Atraves do numero de leaf nodes finais (terminal nodes)(T) e  y estranho(gamma). Estimulo, porque ele corta mesma com valor 0 que nem exposto na aula de classificacao e regressao. Retirei, porque pruning e so no finalzinho e nao serve para nada no processo de derivacao, do Valor Optimal de Output e Similarity Scores. Logo nao e util para a demonstracao de diferenca entre classificao e regressao.

A primeira parte da equacao (da somatoria), nao e nada mais do que a loss function apresentada. A segunda parte refere-se, aos termos de regularizacao.

O O**2, seria o valor de output ao quadrado. O objetivo, dessa equacao e exatamente achar um valor de output do leaf node, que minimize a equacao por completo.O simbolo estranho e o lambda.Mais tarde demonstrarei, como ele minimiza o valor otimal de output(importante).

No exemplo, iremos otimizar o valor de output  para a primeira arvore. Logo pi = 0,5(Predicao inicial)+O(valor de output da primeira arvore depois do datapoint passar por ela, no primeiro exemplo=0)
Ver exemplo:(Equacao da primeira arvore)

No exemplo, lambda=0. Logo todo aquela parte de regulurizacao some.
A loss function,com tudo isso em vista = 104.4, quando o valor de output da primeira arvore = 0.

Vamos montar um grafico para notar a associacao que existe, entre a loss function total(eixo y) e os valores de output(eixo x). Ver imagen:(Funcao de associacao entre output e loss function)

Nesse grafico, existe dois pontos o ponto primario quando o nosso output =0, e a nossa loss function = 104.4, e o segundo que e o resultado de nossa loss function quando o valor de output da arvore =-1(a gente so pois esse valor).Vamos notar, que a loss function subiu logo o output -1 e um merda. Agora se o O fosse = 1, nossa loss function diminuiria de valor o que e uma delicia.

Se fizermos multiplos pontos iremos, obter a seguinte funcao.
Ver imagen:(Funcao de multiplos pontos, com loss function e output)
Nota-se que o ponto otimal de output(ponto com menor loss function), seria onde a derivada=0.
No entanto, lembre-se que essa funcao ocorre somente quando lambda=0.

Se lembra que eu disse, que ia demonstrar como o lambda quando aumentado, diminui o valor otimal de output, entao a imagen sucedente ira demonstra como a medida que eu aumento lambda, o ponto minimo da parabola(valor otimal de output, derivada=0) da funcao de loss function, fica cada vez mais perto de 0. O que e exatamente, o que uma penalidade de regularizacao(lambda) deve fazer.
Ver imagen:(Valor otimal afetado por lambda)

Agora so uma coisinha, a mais antes de acharmos o valor otimal de output.

Voce se lembra la no boost de gradiente nao extremo, existia duas tecnicas matematica para resolver a equacao da imagen. Ver imagem:(Funcao de construcao de arvores, so que sem a parte de T e Gamma). 
Uma para regressao, outra para classificacao. Porque na regressao a matematica para resulucao dessa equacao era suave, agora na classificacao a matematica era um cu.

Especificamente, para classificacao o gradiente de boost nao extremo usava de uma aproximacao de taylor de segunda ordem. Para simplificar a matematica, quando resolvendo a funcao da imagen acima, para achar valor de output otimal. Naquela funcao

Entao, no XGBOOST a gente usa a aproximacao de segunda ordem de taylor, no dois eventos :).

Infelizmente, eu sou muito burro entao. So vou explica basicamente o que e cada parte dessa infeliz equacao. Ver imagen:(Aproximacao de segunda ordem de taylor apropriada para a loss function o valor de outputs)

Primeiro, a parte inicial antes do sinal de aproximado(~~ so que um embaixo do outro). Se refere a loss function, que inclui o valor de output. 

Essa mesma, pode ser aproximada a aquela porrada de derivadas e somatorias. Essa e a magia, de taylor ela divide em um monte de partes simples.

A primeira parte dessa porrada de derivada, (L(y,pi)) seria a loss function da predicao passada. A parte sucente([d/dpiL(y,pi)]), e a primeira derivada daquela loss function. A terceira parte,([d**2/dpi**2L(y,pi)]), e a segunda derivada.

Nota:Ja que a derivada de uma funcao, esta relacionada a algo chamado Gradiente. XGBOOST, usa g para representar as derivadas da loss function. Entao essa parte([d/dpiL(y,pi)]), sera trocada por um g. E ja que a segunda derivada de uma funcao e chamada de Hessian, a gente troca isso daki([d**2/dpi**2L(y,pi)]) por h.

VAMOS EXPANDIR a somotoria e somar a regularizacao. Ver imagen:(Expansao da somatoria + regularizacao)

Vamos substituir L(y1,p1+Ovalue)(loss function que inclui o valor de output), pela aproximacao de segunda ordem de taylor para a loss function de cada ponto, ja simplificada(g e h incluidos). 
Ver imagen:(Segunda ordem de taylor, substiuindo a loss function com relacao ao valor de output)

Antes de continuarmos, lembremos que o que estamos tentando encontrar e um valor de output que minimize a loss function com a regularizacao incluida. Por isso esse calculo todo.
E tudo que a gente fez ate agora, foi pegar a funcao de construcao de arvores, so que sem a parte de T e Gamma, que e o que a gente ta tentando minimizar  e aproximamos, ela a uma segunda ordem de taylor. Ver imagen:(Resuminho honhon)

Voltando ao assunto, os temos que nao tem valor de output, vao ser jogados fora. Por que eles nao tem nenhum efeito, na procura pelo valor otimal. Logo podemos omiti-los da optimizacao,
Ver imagen:(Jogando fora termos nao relacionados)

Agora vamos combinar multiplicados por O com os outros termos tambem multiplicados por O, e os termos multiplicados por O**2 com os multiplicados por O**2. (Fatoracao). Ver imagen:(Fatoracao dos termos multiplicados)

Agora vamos fazer algo que geralmente fazemos quando queremos um valor que minimiza uma funcao, primeiro pegar a derivada com respeito ao valor de output(variavel minimizadora), 2 iguale a derivada a 0, 3 resolva o valor de output. Ver imagen:(Minimizando a loss function)

Depois de fazermos tudo isso, chegamos nisso daki. Ver imagen(Valor de ouput minimizador).

E agora finalmente, chegamos no valor de output ideal para o leaf node.

Agora, temos que plugar os gradientes(g), e os hessians(h) das loss function.

Fazemos isso atraves da seguinte, loss function.Ver imagen:(Loss function de gradiente nao extremo)

Essa loss function, nos possibilita a achar facilmente o resultado final da primeira derivada(g) e da segunda(h).

No caso do g a primera derivada nao sera, nada mais do que a diferenca entre dado observado-dado predito so que negativada. E boom substituimos, dentro da funcao da imagen(Valor de ouput minimizador). Ver imagen:(Plugando o g).

Depois de arrendormos um pouco a formula exposta no plugando o g, notamos que a primeira parte nao e nada mais que a Soma dos residuos, da funcao da imagen(Valor de output minimizador).

Agora fazemos o mesmo esquema com h, pegamos a segunda derivada em relacao a loss function de gradiente nao extremo. E boom obtemos o numero 1. Ver imagen:(Hessian sendo plugada)
Lembrete: Nao se esqueca do lambda no denominador.

No final notamos, que a formula do output ideal de um leaf node,chega na formula de valor de ouput de regressao

Valor de output= Soma dos residuos/Numero de residuos+ lambda


Para resumir o que a gente fez ate agora:

1-Pegamos um databank

2-Fizemos uma predicao inicial qualquer.

3-Ae perguntamos qual deveria ser o valor de output dessa folha? Tendo em consideracao a loss function e a regularizacao(equacao de construcao de arvores).

4-Depois fizemos um grafico, que seria a equacao de construcao de arvore montada como uma funcao do valor de output(Aquela parabola). E resolvemos onde a derivada=0. E obtemos, a fomula de valor de output ja exposta na aula de regressao.



Agora vamos para classificacao

A loss function usado para classificacao com arvores xgboost e a seguinte. Ver imagen:(Funcao de loss classificacao)

Calcular a primeira e a segunda derivada dessa funcao, demora um belo de um tempinho porque o valor dos outputs esta no termo log(odds). Os calculos para achar as derivadas dessa funcao foram feitos na aula de Boost de gradiente- passo a passo para classificacao. Logo nao vou refaze-los mas so para lembrete.Segue o gradiente e o hessian da funcao.
Ver imagen:(Gi e Hi de classificacao)

Depois disso repetimos o processo de substituicao de Gs e Hs dentro da derivada da equacao de construcoes da arvore feita para achar o valor minimo do output(ideal).
Isso vai acabar resultando na mesma formula de valor de output para classificacao tambem achada na aula de xgboost para classificacao.


Valor de output= Soma de residuos//Somatoria[Probabilidade anteriori*(1-Probabilidade anteriori)+lambda]

Lembrete: E PROBABILIDADE PREDITA NAO PROBABILIDADE LOG(ODDS) TEM QUE CONVERTER.

E pronto chegamos na formula de valor de output para xgboost.

Aviso:Isso sao as loss function mais utilizadas, tudo que eu  fiz foi obter as derivadas das loss functions e plugar elas na forma do valor de output ideal pela imagen:(Valor de output minimizador)

Agora precisamos derivar as equacoes para os Similarity Scores, para que a gente possa crescer as arvores.

Xgboost se utiliza da seguinte equacao para determinar o Similarity Score. Ver imagen:(Formula para obter similarity score em xgboost)


A primeira coisa que o xgboost faz e multiplicar essa formula inteira por -1.

Agora a parabola que e utilizada para obter o ponto ideal do output, e invertida.Ver imagen:(Inversao da parabola)

Interessantemente quando ouver essa inversao, a gente vai notar que O valor otimal de output e representando pelo eixo x do maior ponto da parabola. E o eixo y, do maior ponto da parabola, e o Similarity Score.
Pelo menos o dado peloa manuscrito. No entanto, o utilizado na pratico e duas vezes esse eixo y. O porque a gente vai entender ao fazermos a algebra.

Agora vamos obter a formula dos similarity score, que a gente viu na aulas anteriores.

A primeira coisa que a gente faz e plugar o Ovalue(Imagen:Valor de output minimizador) na funcao dada pela (Imagen:Formula para obter similarity score em xgboost). Ver imagen:(Passo 3 para obter similarity score)

E depois de resolvermos a matematica algebraica chegamos nisso. Ver imagen:(Formula do similarity score nao formatada)

No entanto, 1/2 e omitido porque o Similarity Score e um valor de medicao relativo. E contanto que qualquer Similarity Score seja escalado(multiplicado) pelo mesmo valor os resultados vao ser iguais. Isso e um exemplo, do como Xgboost vai fazer de tudo para diminuir a computacao.

Agora se estivermos utilizando a loss function de regressao. E plugarmos o gradiente e o hessian dela na (Imagen:Formula do similarity score nao formatada) sem o 1/2. Vamos obter exatamente a formula expressa do Similarity Score na aula Xgboost para regressao.

Similarity score = (Soma dos residuos) ao quadrado/Numero de residuos+ Lambda) 


Agora se substituirmos, pela loss function de classificacao derivarmos e plugarmos os g e hs na(Imagen:Formula do similarity score nao formatada) sem o 1/2, obtermos  a mesma formula de Similarity Score dado na aula Xgboost para classificacao.

Similiraty Score= (Soma dos residuos) ao quadrado//
Soma[Probabilidade anteriori*(1-Probabilidade anteriori)]+lambda

Agora so um detalhe irritante, o COVER. O cover nao e nada mais do que a somatoria dos hessians dentro da (Imagen:Formula do similarity score nao formatada), tendo isso em visto notamos que ele  e o numero de residuos em uma folha na regressao. 
So que em classificao ele e igual a.

Soma[Probabilidade anteriori*(1-Probabilidade anteriori)]


Conclusao final:

XGBOOST CONSTRUI UMA ARVORE BASEADA NISSO DAKI (IMAGEN:Funcao de construcao de arvores, so que sem a parte de T e Gamma)

A equacao consiste de uma loss function, e uma regularizacao.

E a gente resolve para acharmos o valor de output ideal, para cada leaf node. 

E depois de obtermos-o a gente pluga ele na (Imagen:Formula para obter similarity score em xgboost)


Depois configuramos, de acordo com a loss function utilizada para regressao e classificacao. Plugando os gradiente(primeira derivada) e hessians(segunda derivada), da loss function trabalhada.



























