Em geral uma arvore de decisao, faz uma afirmacao. E depois disso monta uma decisao, caso aquela afirmacao seja verdadeira ou falsa.

Quando uma arvore de decisao, classifica entre categorias(leaf nodes categoricos ou booleanos) e uma arvore de classificacao.

Exemplo, Voce tem uma alta chance de roubar. Se sim voce e negro se nao voce e branco.

Quando uma arvore de decisao, predita um numero. Ela e chamada de arvore de regressao. Exemplo, o rato tem uma dieta especial. Se sim ele vai ter 150 kg para cima, se nao ele vai ter menos do que 150 kg(leaf nodes numericos).

Vamo focar nas de classificacao agora.

Fatos interessantes: Voce pode fazer multiplos limiares(thresholds diferentes). E as ultimas classificacoes podem ser repetidas. E geralmente, se utiliza de verdadeiro e falso, ao decidirmos a classificacao.

O nome do primeiro node(Primeira pergunta) e node raiz, ou root node.
Os nodes que se sucedem, sao chamados de nodes internos ou galhos.
Galhos, tem flechas entrando neles, e flechas saindo deles.(Verdadeiro ou falso decisoes)
Os finais sao chamados de folhas, ou leaves. Leaves tem flechas entrando neles, mas nao saindo.


Agora a pergunta e como a gente vai escolher o root node? Se voce pensar uma database tem multiplos parametros classificativo, exemplo gosta de pipoca, ou gosta de soda, ou gosta de chupar um cu. Como eu escolho aquele que melhor prediz, os outros? Bom o que a gente faz e pequenas arvores de decisao, que equiparam o resultados de cada uma ver imagem(Como escolher root node). Como se pode ver na image, se acontabiliza o numero de vezes que um certo dataset chegou em certo leaf node.Aviso: Quando  o leaf node, tem mais de uma classificacao final, chama-se de node impuro. Tendo isso em vista a arvore com menos nodes finais impuros, e a escolhida. Mas isso nao e muito por assim dizer quantitativo. 

Por isso a gente se utiliza de indice de gini de impuridade, pelo menos esse e o mais famosinho.

A formula de gini de impuridade na folha e
Gini de impuridade na folha=1-(Probabilidade de sim de um leaf node)**2=(Probabilidade de nao de um leaf node)**2

No entanto, a impuridade de gene tambem tem que considerar a disproporcao entre o numero de pessoas, ou eventos em cada leaf node.

Por isso a gente calcula a media dos pesos dos leaf nodes

Lembrete: Peso e o total numero de um leaf node/ pelo  total numero de evento nos leaf nodes comparados.

A formula da impuridade total de gini.
Impuridade total de gini=(numero de eventos da arvore1/numero total de eventos).gini de impuridade da folha1+(numero de eventos da arvore2/numero total de eventos).gini de impuridade da folha2

Para entender melhor ver imagem exemplo(Exemplo gini de impuridade total)


Agora como se faz para classificantes numericos?
Bom e um pouquinho mais convuluido entao mais exemplo.
Imagine um dataset cheio de idades.

A primeira coisa que a gente faz e calcular a media entre as linhas.
E depois calculumos o impuridade total de gini, da arvore de decisao tendo como base a media da linha.Ver exemplo(Exemplo gini de impuridade numero)
Depois disso selecionamos a media,com o menor indice de impuridade total.


E no final selecionamos aquele, o dataset com o menor indice de gini.

No entanto, as vezes mesmo selecionando o root node com o menor indice de gini, ainda temos leaf nodes impuros.Para diminuir isso acrescentamos galhos.

Para isso fazemos um processo semelhante de selecionar o grupo com o menor indice de gini, mas considerando somente os dados que sobraram pos o filtro do primeiro divisor. Ate chegarmos em leaf nodes puros.

Depois disso, a gente conver os leaf nodes para conclusoes(outputs). Ver imagen 
(Transformando para output).Basicamente, formamos a conclusao tendo em consideracao a quantidade de dados afirmativos ou negativo.

E pronto podemos preditar os habitos do individuo tendo em consideracao uma serie de dados

No entanto, a gente pode notar que certas vezes por exemplo em certos classificantes de idade, temos o dado de somente um individuo. O que significa que ao montarmos a nossa arvore de decisao e chegarmos no leaf node puro, generalizamos excessivamente. Afinal, formamos um gini de impuridade com somente um evento. O que seria basicamente overfitting da data.

Para evitar isso se utilizamos de duas metodologias uma e chamada pruning(tem aula) a outra e impondo limites, de quantos dados,eventos, pessoas cada leaf node precisa ter. O problema e que ao fazermos a imposicao de limites, nossos outputs as vezems ficam impuros o que e um belo de um nono.
E para saber o limite ideal, se utilizamos de cross validation. Ver aula cross validation.
