Para entender como o boost de gradiente funciona, internamente. Temos que saber como funciona gradientes descendentes.(Ver aula)

E ja que isso daki vai ter muita formulinha vou explicar cada passo a passo na imagem.Ver imagem:(Passo a passo gradient boost)

Input:
Essa linha, e um jeito abstrato de demonstrar a parte de treinamento do dataset atual, e o metodo que iremos nos utilizar para avaliar o nosso modelo,(nossa loss function). Aquela parte na linha da imagem que tem Data e um monte de simbolos sucessivo seria um jeito chique de demonstrar nosso dataset. X sub i(Xi), se refere a cada linha do dataset. Ver imagen:(X sub i linha 3 e dataset utilizado)  e o Y sub i (Yi) se refere ao peso(weight) do dataset da imagem anterior(Ou melhor, o ponto observado). Aquele finalzinho que tem n e i=1, seria que vai de 1 ate n (sendo n o numero de pessoas avaliadas, nesse caso 3). Juntando todos esses aspectos, e a gente entende que quando estivermos na linha 1, o X sub i, sera = X1, e  o Y sub i = Y1.

Agora em relacao da parte loss function, nessa parte a loss function utilizada seria:

Loss function = 1/2(Dado observado - Dado predito)**2

Se voce prestar atencao se vai notar que essa loss function e bem semelhante a  loss function dos residuos ao quadrado, muita utilizada nos modelos lineares. Isso e porque ela e literalmente a mesma. SO QUE DIVIDIDA POR 1/2 porque? Para facilitar a matematica. Quando a gente faz aquele processo de fazer a derivada, e montar a chain rule. 
Mas e aquele L(yi,F(x)), bom o y sub i seria o y de certo datapoint. E o F(x) seria a funcao que nos da os valores preditos. 
Aviso: Vamos falar sobre ela mais tarde.

A a gente tambem sabe que a loss function e diferenciavel porque somos capazes de fazer a derivada em cada ponto. Diferenciavel, significa que qualquer ponto da funcao, nesse caso da loss function, pode ter a sua derivada(tangente) retirada.

Passo 1:
Bom a gente comeca inicializando nosso modelo com um valor constante.

Aquele bagulho estranho na imagem(Passo a passo gradient boost) no finalzinho do passo 1, e mais legivel se a gente ler da direita, para a esquerda. A parte L(Yi,y) e somente a loss function. Y sub i(Yi), ja foi explicado.	E aquele y seria um gamma, ele se refere ao valores preditos(valores dentro da funcao).

Aquele simbolo de somatoria significa que a gente adiciona uma loss function, para cada valor observado. Ou melhor dita, faz a somatoria da metade dos residuos ao quadrados de TODOS OS DATAPOINTS. Ver imagem(Simbologia da somatoria).

E aquele argmin sobre gamma, significa que a gente precisa achar um valor predito(Predicted, na imagem Simbologia da somatoria), que minimiza a somatoria da metade dos residuos ao quadrado(Sum(1/2)(SR**2)).
Aviso: Podemos ate fazer gradiente descente, para achar o valor optimal. Mas a gente tambem so pode resolver porque a matematica ta easy.

A primeira coisa, que fazemos e retirar a derivada de cada datapoint com respeito a predicao. Eu nao te falei que simplificava para caralho a derivado aquele 1/2 na loss function. Olha la? 
Ver imagem:(Porque usamos metade nas loss function)
Voce viu que a gente perdeu a potencia e so negativo a diferenca entre observado e predito?

Depois disso, a gente iguala a soma das derivadas a 0. E resolve. Ver imagem(Soma das derivadas igualada a 0 resolvidas). 

Veja como a gente acabou com a media dos datapoints observados (no exemplo pesos).

Ou seja, tudo aquela simbologia significa que tendo em consideracao a loss function dada. O primeiro ponto predito(F0(x)), sera nada mais nada menos do que a media dos y sub is.

Se voce se lembra da aula passada, se vai notar que isso e o desenvolvimento do leaf node inicial. 

E pronto acabamos o passo 1, inicializamos o nosso modelo com a media dos y sub is.


O passo 2 e gigante, por isso vamos dividir em a, b , c, d.

Passo 2:

O passo 2 e uma simbologia para todas arvores sucessivo a arvore que e somente um leaf node. Por isso ele e representado como um loop, onde M seria o numero total te arvores existente e m o numero de cada arvore individual. No exemplo, m=1 porque se refere ao comeco do loop ou melhor a primeira arvore de verdade feita.

A)
Na frase da letra A a gente ve um monte de simbolos estranhos. Isso seria o processo que resulta no residuo(Dado observado - dado predito), ele chega nessa formula depois de derivar e negativar artificialmente. Ver imagem:(Simbologia letra a)

Aquela parte F(x)=Fm-1(x), seria o F0(x) no exemplo ou a arvore que consiste de somente um leaf node. Lembrando que isso SE REFERE A PRIMEIRA ARVORE(m=1, SE FOSSE m=2, seria o residuo pos segunda arvore e  arvore leaf node).

O ri,m, na letra a. O r se refere ao residuo, i seria o numero do datapoint. E m seria a arvore que a gente ta construindo.
Ou seja r1,1, seria o residuo do primeiro datapoint(dado observado) comparado a primeira arvore(dado predito)

Aquele for i= 1,.... n significa, calcular o residuo de todos os datapoints.

E pronto calculamos o pseudo-residuo de cada datapoint. Obviamente com a loss function, 1/2(Observado-Predito)**2. Se a gente usasse outra

B)
A frase da parte B no geral, esta dizendo que vamos construir uma arvore de regressao. Para preditar os residuos ao inves dos pesos.

A parte sucedente referese a criacao de label para as regioes terminais(leaf nodes). m seria o index da arvore, no exemplo dado m=1. j index para cada folha na arvore. Jm, serio o numero total de leaf nodes da arvore formada tendo os residuos como base. So para treinamento se a PRIMEIRA arvore treinada de acordo com os residuos tivesse 4 leaf nodes, o label de cada leaf node seria. Em ordem numerica, R1,1,R2,1,R3,1,R4,1

E boom terminamos a parte  B ,treinamos uma arvore de acordo com os residuos e pusemos labels nos leaf nodes.

C)
Na parte C determinamos o valor de output(residuo de cada datapoint) de cada folha.

Essa parte e excessivamente complexa para explicar passo a passo, basicamente se refere aquele processo que a gente faz a media dos residuos de certos datapoint que "caieram naquela folha".

D)

Atualizacao da nova predicao que seria, o pseudo residuo do leaf node + learning rate*residuo do leaf node depois do datapoint ser classificado pela arvore + learning rate* ..... (dependendo do numero de interacoes). Isso dae seria a predicao do dado observado.Ver imagen:(Imagem output de predicao quando M =3)

O passo 3:
E nada mais que a simbologia do output desse processo.

Aviso: Chato pra caralho