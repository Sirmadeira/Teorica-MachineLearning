Se lembra que no inicio de tudo(Aula 11), eu falei que xgboost era um algoritmo grande com multiplos passos entao. Tudo que desenvolvemos ate agora era o passo a passo da construcao da arvore especial de xgboost, e seus detalhes matematicos.

Agora vamos para os outros passos.Especialmente os que sucedem, o passo 3 (de construcao da arvore unica). Ver imagen:(Passos de xgboost)

Esses passos sucedentes, sao o que fazem do xgboost relativamente eficaz. Com datasets grandes.Logo eles sao passos optimizadores.

Os passos anteriores, ja erao mais enfocados para dar uma ideia do como xgboost e treinado em datasets e como ele faz predicoes.

Vamos comecar pelo Aproximate Greedy Algorithm.

Se lembra quando a gente construi a primeira arvore de xgboost, e a nossa selecao dos galhos e baseado no nosso Gain. Especificamente, seleciona-se o limiar(threshhold) residual, com o maior Gain.

Entao, essa decisao nao leva em consideracao. Se os nossos leaf nodes, vao ser cortados(prunados) posteriormente.Tanto que o passo e feito antes mesmo do processo de pruning por Gamma.(Gain de um galho-Gamma)

Ele tambem nao leva em consideracao, se os galhos sucedentes vao ter limiares que seriam melhores no longo prazo.

Isso significa que Xgboost se utiliza de um Greedy algorithm, para construir a sua arvore. Ele faz uma decisao sem saber se ela e exatamente a melhor decisao possivel.

Em outras palavras ele construi arvores rapidamente.

No entanto, mesmo com um algoritmo greedy, ele ainda teria que avaliar se todas as possibilidades das variaveis(colunas) dos datapoints sao validas o que ainda demoraria muito.

E ae que vem a ideia de aproximado.	Ao inves de nos utilizarmos da dataset por completo a gente dividiria ele e so avaliaria os quantiles. Como limiares candidatos. Ver imagen:(Dataset dividido em quantiles)

Se a gente somente tivesse 1 quantiles, treinariamos muito rapido(porque nao precisaria calcular Gain e Similiraty). So que nosso root node seria somente ele(porque ele seria o unico limiar de formacao disponivel). O que daria um modelo de precisao muito baixa.
Se tivessemos mais, teriamos mais limiares para ser testados e selecionados so que demoraria mais tempo. E um famos tradeoff.

Resumidamente, o Approximate significa que ao inves de testarmos todos os threshholds possiveis a gente so testa os quantiles. Geralmente, em pratica ele se utiliza em torno(importante) de 33 quantiles de avaliacao. Esse em torno e importante, por que nao nos utilizamos de exatamente 33 quantiles.

Para respondermos isso temos que avaliar os passos seguintes, especificamente Parallel learning e Weighted Quantile Sketch.

Imagine que a gente tem um dataset tao grande mas tao grande, que a memoria inteira de um computador nao e capaz de lidar com eles. Coisa simples como, filtrar uma lista ou achar quantiles se torna bem lenta.

Para fugir desse problema, uma classse de algoritmos chamados Sketches podem rapidamente criar solucoes aproximados.Nao vou explicar os Algoritmos de Sketches a fundo.

Mas vou dar uma nocao geral de como eles funcionam, atraves de um exemplo.Ver imagen:(Dataset para conceitos base de sketches)

Imagine que pegamos o dataset da imagen:(Dataset para conceitos base de sketches), e dividimos ele em multiplos mini datasets e pomos eles em multiplos computadores. 

O Quantile Sketch Algorythm, combina os valores de cada computador para fazer um histograma aproximado.

Depois, o histograma aproximado e usado para calcular os quantiles aproximados e depois o Approximate Greedy Algorythm, usa dos quantiles aproximados. Ver imagen:(Desenvolvimento algoritmo de sketches)

No entanto, voce viu que eu descrevi o Quantile Sketch Algorythm, nao o Weighted Quantile Sketch Algorythm.

No Weighted Quantile Sketch Algorythm, os quantiles nao sao quantiles normais. Ao inves disso, eles sao quantiles com pesos.

Qual a diferenca entre os dois? Os quantiles normais, sao conhecidos por dividir a data em partes iguais, ou seja, se tiver 50 datapoints. E eu fizer um quantile, cada lado vai ter 25 datapoints. Agora nos quantiles com pesos, cada datapoint, tem um peso correspondente e se define eles atraves da soma dos pesos que tem que ser igual. 

Ou seja, o quantile 1 vai ter a mesma soma dos pesos dos datapoints, do que o quantile 2. Mas nao necessariamente, a mesma quantidade de datapoints. Na regressao ainda e igual o motivo porque segue.

Os pesos de cada datapoint, sao  definidos pelo Cover. Discutidos nas aulas anteriores. Especificamente, o peso para cada observacao e a segunda derivada da loss function.(famoso hessian). Isso significa, que para regressao os pesos sao todos iguais a 1. O que resulta, com que cada quantile mesmo com peso tenha o mesmo numero de observacoes.

No entanto, em classificacao os pesos vao ser.

Peso = Probabilidade anteriori*(1-Probabilidade anteriori)

Vamos ver como a equacao para os pesos, afeta os quantiles em classificacao. E faremos isso atraves do seguinte dataset. Ver imagen:(Dataset pos treino). Esse dataset, seria o dataset com alguma arvores ja formuladas, e com residuos menores.

Isso informacao e importante, porque os pesos como a formula ja demonstra sao diretamente associados, as probabilidade anteriores. E ao calcularmos as probabilidades anteriores, de datapoints que estao bem treinados(baixo residuos, proximos do dado observado, e com probabilidade anteriores alta). Notamos que o peso deles, e menor. 
Enquanto datapoints mau treinados(probabilidade anterioress baixas, longe de dados observados, baixos residuos) tem peso grandes. Ver imagem:(Peso dos datapoints de classificacao.)

Interessantemente, tambem somos capazes de notar que caso dividissemos igualmente os quantiles na classificacao. Os datapoints com pesos grandes e residuos semelhantes, na formula do valor de output iriam cair no mesmo leaf node e se cancelar. O que impossibilita e ate desacelera, o treinamento. Por isso, a divisao por soma dos pesos e mais promissora. Por que, conseguimos quantiles menores quando precisamos deles.


Resumindo:

Aproximate Greedy Algorithm, seria a definicao do limiar divisor residual na formacao de galhos. Antes de prunar, a arvore.

Parallel learning, seria a divisao dos datasets em datasets menores. E seu treino, em computadores separados.

Weighted Quantile Sketch, combina os dados num histograma aproximado. E esse histograma, e dividido em quantiles que poim as observacoes(datapoints) de baixa confianca(probabilidade anterior baixa) em quantile com menos observacoes. Para assim, melhor treina-los. 

Nota: Ele so usa os 3 quando o dataset e enorme. Quando e pequeno, so usa o greedy.

Vamos falar Sparsity Aware Split Finding.

Imagine um dataset com dados vazios.Mesmo com dados vazios ainda, somos capazes de calcular os residuos dos dados perdidos. Com a predicao inicial(0.5). Ver imagen:(Dataset sparsity)

E exatamente como antes, podemos comecar a construcao de nossa arvore com pondo esses residuos no root node.

Agora, precisamos determinar qual dos quantiles de residuos sera escolhido para a divisao dos novos leaf nodes(Selecao atraves de gain). E isso, necessariamente obriga que a gente alinhe os dados de baixo para cima. O que e impossivel, porque dados Nan(Nao disponiveis) nao sao filtraveis.

Para evitar isso, separamos os dataset entre que tem dados e quem nao tem.E logo em seguida, refazemos o processo de selecao dos limiares residuais. Ver imagen:(Formacao arvore de xgboost sem dados).

No entanto, ao calcularmos o Gain o Similarity Score. A gente encaixa todos os dados do dataset vazio dentro do leaf node que esta tendo seu dado calculado . Ver imagen:(Calculos dos Gains em xgboost parametrizado). E no final, selecionamos o leaf node que teve o maior dos Gain. E esse sera aquele, que os residuos dos dados vazios vao estar.

Se eu tivesse um dado totalmente vazio, que eu devesse fazer a predicao de, eu assumiria que esse dado ao passar por minha arvore, iria ter o seu valor de output como o do leaf node que tem os dados vazios(ele cai naquele leaf).

Isso e o que Sparsity Aware Split Finding, seria a maneira como tratamos os nossos dados vazios.

Vamos para Cache Aware Access

A ideia basica do Cache Aware Acess, comeca com um computador e seu CPU. O CPU tem uma parte chamada Cache memory o CPU pode se utilizar dessa memoria mais rapido que qualquer outra memoria no computador. Essa CPU, esta tambem atracada a uma quantidade maior de memoria chamada Main memory, ela e maior so que demora mais para usar. E por fim tem o Hard Drive mais lento de todos.

O xgboost, pega o Cache Aware Acess, e poim os Hessians e Gradientes nele para calcular os Similarity Scores e Output mais rapidamente.

Por fim, vamos Block for out of core computation.

Quando o dataset e muito grande para Main memory, e o Cache memory temos que por um poucado no hard drive. No entanto ele e super lento. Entao o xgboost, minimiza essas acoes comprimindo os dados que vao para o HD(hard drive). E mesmo, que ele precise descomprimir a data que vem do HD ele pode fazer isso mais rapido. 

Alem do mais, quando se tem mais de um Hd Xgboost se utiliza de uma tecnica chamada sharding para acelerar o acesso ao disco.

Sharding e quanto voce divide os datasets, para que cada drive possa ler os dados de uma vez so.

No final, Cache Aware Acess e Blocks for out of core Computation sao optimizadores de hardware.

Xgboost tambem acelera, ao selecionar substes randomicos de dados. Ao decider como dividir eles.

Conclusao final:

Xgboost e rapido pra caraio.




 




















 




